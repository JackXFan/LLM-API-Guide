{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "# read config file from root directory\n",
    "config_path = '../config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set up OpenAI API credentials\n",
    "os.environ[\"OPENAI_API_KEY\"] = config['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chat Format Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Response returned by model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The chat completion object example\n",
    "example = {\n",
    "  \"id\": \"chatcmpl-123\",                       # A unique identifier for the chat completion.\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1677652288,\n",
    "  \"model\": \"gpt-3.5-turbo-0613\",\n",
    "  \"choices\": [{\n",
    "    \"index\": 0,\n",
    "    \"message\": {\n",
    "      \"role\": \"assistant\",                    # The role of the author of this message.\n",
    "      \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
    "    },\n",
    "    \"finish_reason\": \"stop\"                   # The reason why the chat stopped.\n",
    "  }],\n",
    "  \"usage\": {                                  # Usage statistics for the completion request.\n",
    "    \"prompt_tokens\": 9,\n",
    "    \"completion_tokens\": 12,\n",
    "    \"total_tokens\": 21\n",
    "  }\n",
    "}\n",
    "\n",
    "#### The chat completion chunk object example\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"!\"},\"finish_reason\":null}]}\n",
    "\n",
    "# ....\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" today\"},\"finish_reason\":null}]}\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"?\"},\"finish_reason\":null}]}\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create chat completion (request and response):\n",
    "Full information and parameters: [Click Here](https://platform.openai.com/docs/api-reference/chat/create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-885yToHojWdXeSF1U2JPzKKg6qUhQ at 0x2a35d5d31d0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"As an AI developed by OpenAI, my underlying model is GPT-3. As of the current moment, a GPT-4 iteration has not been released.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1696940281,\n",
       "  \"id\": \"chatcmpl-885yToHojWdXeSF1U2JPzKKg6qUhQ\",\n",
       "  \"model\": \"gpt-4-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 34,\n",
       "    \"prompt_tokens\": 36,\n",
       "    \"total_tokens\": 70\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No streaming\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  # temperature=0.9,  # between 0 and 2, higher -> more diversity\n",
    "  # top_p=1,         # between 0 and 1, higher -> less diversity. Altering this or temperature but not both is recommended\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello! What's your model? gpt-3.5 or gpt-4?\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message)\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"Hello\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"!\"\n",
      "}\n",
      "{\n",
      "  \"content\": \" How\"\n",
      "}\n",
      "{\n",
      "  \"content\": \" can\"\n",
      "}\n",
      "{\n",
      "  \"content\": \" I\"\n",
      "}\n",
      "{\n",
      "  \"content\": \" assist\"\n",
      "}\n",
      "{\n",
      "  \"content\": \" you\"\n",
      "}\n",
      "{\n",
      "  \"content\": \" today\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"?\"\n",
      "}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  print(chunk.choices[0].delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"function_call\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\",\n",
      "          \"name\": \"get_current_weather\"\n",
      "        },\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1696940295,\n",
      "  \"id\": \"chatcmpl-885yhzWPQMglofgypg9S1FFgevTjW\",\n",
      "  \"model\": \"gpt-4-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 18,\n",
      "    \"prompt_tokens\": 83,\n",
      "    \"total_tokens\": 101\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "functions = [ \n",
    "  {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Get the current weather in a given location\", # used by the model to choose when and how to call the function.\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "            },\n",
    "            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    "  }\n",
    "]\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  functions=functions,\n",
    "  function_call=\"auto\",  # auto is default, but we'll be explicit. \"none\" means the model will not call a function and instead generates a message. \"auto\" means the model can pick between generating a message or calling a function. Specifying a particular function via {\"name\": \"my_function\"} forces the model to call that function\n",
    ")\n",
    "\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Advanced usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Example of function call:\n",
    "> Function calling allows you to more reliably get structured data back from the model. For example, you can:\n",
    "> - Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)\n",
    ">     - e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: > string, unit: 'celsius' | 'fahrenheit')\n",
    "> - Convert natural language into API calls\n",
    ">     - e.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, > limit: int) and call your internal API\n",
    "> - Extract structured data from text\n",
    ">     - e.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)\n",
    "> \n",
    "> ...and much more!\n",
    "\n",
    "> The basic sequence of steps for function calling is as follows:\n",
    "> 1. Call the model with the user query and a set of functions defined in the functions parameter.\n",
    "> 2. The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).\n",
    "> 3. Parse the string into JSON in your code, and call your function with the provided arguments if they exist.\n",
    "> 4. Call the model again by appending the function response as a new message, and let the model summarize the results back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"The weather in Boston is currently sunny and windy with a temperature of 72 degrees.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1696942437,\n",
      "  \"id\": \"chatcmpl-886XFpNzJDkwvCEHQdwsfQkphqeoo\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 17,\n",
      "    \"prompt_tokens\": 72,\n",
      "    \"total_tokens\": 89\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)\n",
    "\n",
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to GPT\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "    # Step 2: check if GPT wanted to call a function\n",
    "    if response_message.get(\"function_call\"):\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        function_name = response_message[\"function_call\"][\"name\"]\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        function_response = function_to_call(\n",
    "            location=function_args.get(\"location\"),\n",
    "            unit=function_args.get(\"unit\"),\n",
    "        )\n",
    "\n",
    "        # Step 4: send the info on the function call and function response to GPT\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )  # extend conversation with function response\n",
    "        second_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "        return second_response\n",
    "\n",
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
