{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "# read config file from root directory\n",
    "config_path = '../config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set up OpenAI API credentials\n",
    "os.environ[\"OPENAI_API_KEY\"] = config['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chat Format Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Response returned by model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The chat completion object example\n",
    "example = {\n",
    "  \"id\": \"chatcmpl-123\",                       # A unique identifier for the chat completion.\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1677652288,\n",
    "  \"model\": \"gpt-3.5-turbo-0613\",\n",
    "  \"choices\": [{\n",
    "    \"index\": 0,\n",
    "    \"message\": {\n",
    "      \"role\": \"assistant\",                    # The role of the author of this message.\n",
    "      \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
    "    },\n",
    "    \"finish_reason\": \"stop\"                   # The reason why the chat stopped.\n",
    "  }],\n",
    "  \"usage\": {                                  # Usage statistics for the completion request.\n",
    "    \"prompt_tokens\": 9,\n",
    "    \"completion_tokens\": 12,\n",
    "    \"total_tokens\": 21\n",
    "  }\n",
    "}\n",
    "\n",
    "#### The chat completion chunk object example\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"!\"},\"finish_reason\":null}]}\n",
    "\n",
    "# ....\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" today\"},\"finish_reason\":null}]}\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"?\"},\"finish_reason\":null}]}\n",
    "\n",
    "# {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create chat completion (request and response):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No streaming\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  # temperature=0.9,  # between 0 and 2, higher -> more diversity\n",
    "  # top_p=1,         # between 0 and 1, higher -> less diversity. Altering this or temperature but not both is recommended\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello! What's your model? gpt-3.5 or gpt-4?\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message)\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  print(chunk.choices[0].delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "functions = [ \n",
    "  {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Get the current weather in a given location\", # used by the model to choose when and how to call the function.\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "            },\n",
    "            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    "  }\n",
    "]\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  functions=functions,\n",
    "  function_call=\"auto\",  # auto is default, but we'll be explicit. \"none\" means the model will not call a function and instead generates a message. \"auto\" means the model can pick between generating a message or calling a function. Specifying a particular function via {\"name\": \"my_function\"} forces the model to call that function\n",
    ")\n",
    "\n",
    "print(completion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
